{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import numpy as np\n",
    "import torch\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "from torchvision.models import resnet50"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained ResNet-50 model\n",
    "model = resnet50(pretrained=True)\n",
    "model.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define transformations to preprocess images\n",
    "preprocess = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
    "])\n",
    "\n",
    "# Function to extract features using ResNet-50\n",
    "def extract_features(image):\n",
    "    if image is None:\n",
    "        return None\n",
    "\n",
    "    image = Image.fromarray(cv2.cvtColor(image, cv2.COLOR_BGR2RGB))\n",
    "    image = preprocess(image)\n",
    "    image = torch.unsqueeze(image, 0)  # Add batch dimension\n",
    "    with torch.no_grad():\n",
    "        features = model(image)\n",
    "    return features.squeeze().numpy()\n",
    "\n",
    "# Function to compare feature embeddings\n",
    "def cosine_similarity(embedding1, embedding2):\n",
    "    return np.dot(embedding1, embedding2) / (np.linalg.norm(embedding1) * np.linalg.norm(embedding2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load YOLO model and weights\n",
    "net = cv2.dnn.readNet(\"/content/yolov3.weights\", \"/content/darknet/cfg/yolov3.cfg\")\n",
    "\n",
    "# Load coco class labels\n",
    "classes = []\n",
    "with open(\"/content/darknet/data/coco.names\", \"r\") as f:\n",
    "    classes = f.read().strip().split(\"\\n\")\n",
    "\n",
    "# Get output layer indices\n",
    "output_layer_indices = net.getUnconnectedOutLayers()\n",
    "\n",
    "# Get output layer names\n",
    "output_layers = [net.getLayerNames()[idx - 1] for idx in output_layer_indices]\n",
    "\n",
    "# Function to detect persons using YOLO\n",
    "def detect_persons(frame):\n",
    "    height, width, _ = frame.shape\n",
    "\n",
    "    # Resize frame to YOLO input size and normalize\n",
    "    blob = cv2.dnn.blobFromImage(frame, 0.00392, (416, 416), (0, 0, 0), True, crop=False)\n",
    "\n",
    "    # Pass blob through the network\n",
    "    net.setInput(blob)\n",
    "    outs = net.forward(output_layers)\n",
    "\n",
    "    # Process detections\n",
    "    boxes = []\n",
    "    confidences = []\n",
    "    class_ids = []\n",
    "    for out in outs:\n",
    "        for detection in out:\n",
    "            scores = detection[5:]\n",
    "            class_id = np.argmax(scores)\n",
    "            confidence = scores[class_id]\n",
    "            if confidence > 0.5 and class_id == 0:  # 0 corresponds to 'person' class in COCO dataset\n",
    "                # Extract coordinates of bounding box\n",
    "                center_x = int(detection[0] * width)\n",
    "                center_y = int(detection[1] * height)\n",
    "                w = int(detection[2] * width)\n",
    "                h = int(detection[3] * height)\n",
    "                x = int(center_x - w / 2)\n",
    "                y = int(center_y - h / 2)\n",
    "                boxes.append([x, y, w, h])\n",
    "                confidences.append(float(confidence))\n",
    "                class_ids.append(class_id)\n",
    "\n",
    "    # Apply non-maximum suppression to remove overlapping boxes\n",
    "    indexes = cv2.dnn.NMSBoxes(boxes, confidences, 0.5, 0.4)\n",
    "\n",
    "    # Filter detected persons\n",
    "    detected_boxes = []\n",
    "    if len(indexes) > 0:\n",
    "        for i in indexes.flatten():\n",
    "            detected_boxes.append(boxes[i])\n",
    "    return detected_boxes\n",
    "\n",
    "# Function to draw bounding boxes around detected persons with IDs\n",
    "def draw_boxes_with_ids(image, boxes, ids):\n",
    "    for box, person_id in zip(boxes, ids):\n",
    "        x, y, w, h = box\n",
    "        cv2.rectangle(image, (x, y), (x+w, y+h), (0, 255, 0), 2)\n",
    "        cv2.putText(image, str(person_id), (x, y - 5), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (255, 255, 255), 2)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Load and process videos\n",
    "video1 = cv2.VideoCapture(\"/content/5.mp4\")\n",
    "video2 = cv2.VideoCapture(\"/content/4.mp4\")\n",
    "\n",
    "# Initialize variables for storing person IDs\n",
    "person_ids_video1 = {}\n",
    "person_ids_video2 = {}\n",
    "next_person_id_video1 = 1\n",
    "next_person_id_video2 = 1\n",
    "\n",
    "# Select the best frame for reference from the first video\n",
    "reference_frame_data_video1 = []\n",
    "num_reference_frames = 5\n",
    "while len(reference_frame_data_video1) < num_reference_frames:\n",
    "    ret, frame = video1.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    detected_boxes = detect_persons(frame)\n",
    "    for box in detected_boxes:\n",
    "        x, y, w, h = box\n",
    "        person_image = frame[y:y+h, x:x+w]\n",
    "\n",
    "        if person_image.size != 0:\n",
    "            person_embedding = extract_features(person_image)\n",
    "            reference_frame_data_video1.append((person_embedding, box))\n",
    "\n",
    "# Initialize a dictionary to store embeddings of people detected in video 1\n",
    "person_embeddings_video1 = {}\n",
    "\n",
    "# Assign unique IDs to people detected in video 1\n",
    "for embedding, box in reference_frame_data_video1:\n",
    "    if embedding.tobytes() not in person_embeddings_video1:\n",
    "        person_embeddings_video1[embedding.tobytes()] = next_person_id_video1\n",
    "        next_person_id_video1 += 1\n",
    "    person_ids_video1[tuple(box)] = person_embeddings_video1[embedding.tobytes()]\n",
    "\n",
    "# Match persons detected in video 2 with those in the reference frames from video 1\n",
    "while True:\n",
    "    ret, frame = video2.read()\n",
    "    if not ret:\n",
    "        break\n",
    "\n",
    "    if frame is not None:\n",
    "        detected_boxes = detect_persons(frame)\n",
    "\n",
    "        for box in detected_boxes:\n",
    "            x, y, w, h = box\n",
    "            person_image = frame[y:y+h, x:x+w]\n",
    "\n",
    "            if person_image.size != 0:\n",
    "                person_embedding = extract_features(person_image)\n",
    "\n",
    "                # Check if the embedding matches with any of the embeddings from video 1\n",
    "                matched_embedding = None\n",
    "                for ref_embedding in reference_frame_data_video1:\n",
    "                    similarity = cosine_similarity(person_embedding, ref_embedding[0])\n",
    "                    if similarity > 0.8:  # Adjust threshold as needed\n",
    "                        matched_embedding = ref_embedding\n",
    "                        break\n",
    "\n",
    "                if matched_embedding is not None:\n",
    "                    # Check if the matched box is present in person_ids_video1\n",
    "                    matched_box_tuple = tuple(matched_embedding[1])  # Convert list to tuple\n",
    "                    if matched_box_tuple in person_ids_video1:\n",
    "                        # Assign the same ID as the matched person from video 1\n",
    "                        person_ids_video2[tuple(box)] = person_ids_video1[matched_box_tuple]\n",
    "                    else:\n",
    "                        # Assign a new ID for this person\n",
    "                        person_ids_video2[tuple(box)] = next_person_id_video2\n",
    "                        next_person_id_video2 += 1\n",
    "                else:\n",
    "                    # No matching embedding found, assign a new ID for this person\n",
    "                    person_ids_video2[tuple(box)] = next_person_id_video2\n",
    "                    next_person_id_video2 += 1\n",
    "\n",
    "# Reset video captures to process them again\n",
    "video1.release()\n",
    "video2.release()\n",
    "video1 = cv2.VideoCapture(\"/content/5.mp4\")\n",
    "video2 = cv2.VideoCapture(\"/content/4.mp4\")\n",
    "\n",
    "# Initialize variables for video writers\n",
    "frame_width_video1 = int(video1.get(3))\n",
    "frame_height_video1 = int(video1.get(4))\n",
    "out_video1 = cv2.VideoWriter('output_video1.mp4', cv2.VideoWriter_fourcc(*'MP4V'), 30, (frame_width_video1, frame_height_video1))\n",
    "frame_width_video2 = int(video2.get(3))\n",
    "frame_height_video2 = int(video2.get(4))\n",
    "out_video2 = cv2.VideoWriter('output_video2.mp4', cv2.VideoWriter_fourcc(*'MP4V'), 30, (frame_width_video2, frame_height_video2))\n",
    "\n",
    "# Process videos and draw bounding boxes with IDs\n",
    "while True:\n",
    "    ret1, frame1 = video1.read()\n",
    "    ret2, frame2 = video2.read()\n",
    "    if not ret1 or not ret2:\n",
    "        break\n",
    "\n",
    "    if frame1 is not None:\n",
    "        detected_boxes = detect_persons(frame1)\n",
    "        # Get IDs for detected persons in video 1\n",
    "        person_ids = [person_ids_video1.get(tuple(box), 0) for box in detected_boxes]  # Convert box to tuple\n",
    "        # Draw bounding boxes with IDs for video 1\n",
    "        draw_boxes_with_ids(frame1, detected_boxes, person_ids)\n",
    "        out_video1.write(frame1)  # Write the processed frame to the output video\n",
    "\n",
    "    if frame2 is not None:\n",
    "        detected_boxes = detect_persons(frame2)\n",
    "        # Get IDs for detected persons in video 2\n",
    "        person_ids = [person_ids_video2.get(tuple(box), 0) for box in detected_boxes]  # Convert box to tuple\n",
    "        # Draw bounding boxes with IDs for video 2\n",
    "        draw_boxes_with_ids(frame2, detected_boxes, person_ids)\n",
    "        out_video2.write(frame2)  # Write the processed frame to the output video\n",
    "\n",
    "# Release resources\n",
    "video1.release()\n",
    "video2.release()\n",
    "out_video1.release()\n",
    "out_video2.release()\n",
    "cv2.destroyAllWindows()"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
